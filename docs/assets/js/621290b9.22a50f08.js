"use strict";(self.webpackChunkarc=self.webpackChunkarc||[]).push([[9958],{4396:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"/Llama3","metadata":{"permalink":"/arc/blog/Llama3","source":"@site/blog/Llama3.md","title":"Llama3 is out!","description":"Meta have released a new version of their large language model, Llama3, https://ai.meta.com/blog/meta-llama-3/.","date":"2024-11-15T08:57:22.000Z","tags":[],"readingTime":0.35,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Llama3 is out!"},"unlisted":false,"nextItem":{"title":"Sptring.ai integration using the SpringChatClient","permalink":"/arc/blog/Spring.AI-integration"}},"content":"Meta have released a new version of their large language model, Llama3, https://ai.meta.com/blog/meta-llama-3/.\\n\\nThanks to ollama, the model can easily be tested locally, see https://ollama.com/library/llama3\\n\\nSimply pull the model, run `ollama` and start using it with Arc!\\n\\n```bash\\nollama pull llama3:8b\\nollama serve\\n```\\n\\nNow llama3:8b is ready to use with Arc and the ollama client.\\nWe recommend using the 8B variant locally, as it \\"only\\" requires 8GB of RAM."},{"id":"Spring.AI-integration","metadata":{"permalink":"/arc/blog/Spring.AI-integration","source":"@site/blog/SpringAiClient.md","title":"Sptring.ai integration using the SpringChatClient","description":"As the world of AI continues to evolve and develop,","date":"2024-11-15T08:57:22.000Z","tags":[{"inline":true,"label":"spring","permalink":"/arc/blog/tags/spring"},{"inline":true,"label":"Spring.AI","permalink":"/arc/blog/tags/spring-ai"},{"inline":true,"label":"feature","permalink":"/arc/blog/tags/feature"}],"readingTime":2.3,"hasTruncateMarker":false,"authors":[{"name":"Max","socials":{},"key":null,"page":null}],"frontMatter":{"slug":"Spring.AI-integration","title":"Sptring.ai integration using the SpringChatClient","authors":[{"name":"Max"}],"tags":["spring","Spring.AI","feature"]},"unlisted":false,"prevItem":{"title":"Llama3 is out!","permalink":"/arc/blog/Llama3"}},"content":"As the world of AI continues to evolve and develop,\\nthe need of integration of AI services into your applications \\nhas become increasingly valuable.\\n\\nSince ARC\'s goal is to make the process of integration, management \\nand creation of AI in your existing services as seamless as possible \\nit only made sense to integrate Spring.AI to work in a plug and play \\nfashion.\\n \\n\\nIn this blog post, we\'ll explore how to implement a new adapter for \\nSpring.AI to work within ARC.\\nTo expand its capabilities and make it easier to incorporate various \\nAI services into your projects.\\n\\n**What is an the `SpringChatClient` Adapter in ARC?**\\n\\nIn ARC, the `SpringChatClient` acts like a bridge between the framework and \\nexternal Spring.AI models/ APIs. It enables you to seamlessly integrate any \\nSpring.AI ChatModels in your ARC application, allowing for easy access to their \\nfunctionality. These Adapters/ ChatClients can be used to connect to various \\nAI platforms, such as Google VertexAI, Amazon Comprehend, Grog, Mistral.Ai, \\nIBM Watson or many [more](https://docs.spring.io/spring-ai/reference/api/chatmodel.html).\\n\\n**Why use the New Adapter?**\\n\\nThe Adapter allows for quick ruse of existing AI model API\'s written by Spring.AI.\\nThis will allow any developer familiar with Spring.AI to get a head start to further\\nget to know and love the unique features that come with ARC.\\n\\n\\nHowever there are some *limitations* to the use of Spring.AI models in ARC.\\nSince they are not written with re-loadability and DSL in mind not all \\nkey features will work.\\n\\n\\n**Step-by-Step Guide to Implementing a New Adapter:**\\n\\nTo implement a new adapter for Spring.AI, follow these steps:\\n\\n1. **Choose an AI Service:** Select the AI model you\'d like to integrate\\nwith your application. In this case we will re-implement the natively \\nexisting [ollama client](blog/Llama3.md).\\n\\n1. **Get it done:** Since we try to eliminate boilerplate this wont take \\nlong :).\\n\\n```kotlin title=\'chatCompleterProvider for Ollama\'\\npackage io.github.lmos.arc.Spring.AI\\n\\n// Reusing Spring.AI models and terminologies\\nimport org.springframework.ai.ollama.OllamaChatModel\\nimport org.springframework.ai.ollama.api.OllamaApi\\nimport org.springframework.ai.ollama.api.OllamaOptions\\nimport org.springframework.boot.autoconfigure.SpringBootApplication\\nimport org.springframework.context.annotation.Bean\\n\\n@SpringBootApplication\\nopen class YourApplication {\\n\\n    @Bean\\n    open fun chatCompleterProvider(ollamaApi: OllamaApi) = SpringChatClient(\\n        OllamaChatModel(\\n            OllamaApi(\\"http://localhost:8888\\"),\\n            OllamaOptions.create().withModel(\\"llama3:8b\\")),\\n            \\"llama3:8b\\",\\n    )\\n\\n}\\n```\\n**Conclusion:**\\n\\nUsing the new adapter for Spring.AI `SpringChatClient` can greatly reduce\\nthe time of implementation for anyone who has used Spring.AI before. \\nBy following the before mentioned steps, you can integrate any of the \\nSpring.AI models that cater to the specific needs of your applications.\\n\\nRemember to choose an AI service that aligns with your project\'s goals and\\nrequirements, and don\'t hesitate to reach out if you have any questions or\\nneed further guidance on implementing a ARC agent. Happy coding!\\n\\n:::tip full-potential\\nBe sure to create a custom implementation of the `ChatCompleter` interface or \\nany of the many predefined chatCompleters within the ARC repo to unleash the\\nfull potential of the nimble ARC framework for agent creation.\\n:::"}]}}')}}]);